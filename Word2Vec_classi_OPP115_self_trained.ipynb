{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import scipy.sparse as sps\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import re\n",
    "import os\n",
    "import shutil\n",
    "import sys\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from collections import Counter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import Normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install scikit-learn==0.23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skmultilearn.problem_transform import LabelPowerset # pip3 install scikit-multilearn\n",
    "from imblearn.over_sampling import RandomOverSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>website</th>\n",
       "      <th>alltext</th>\n",
       "      <th>segment ID</th>\n",
       "      <th>Data Retention</th>\n",
       "      <th>Data Security</th>\n",
       "      <th>Do Not Track</th>\n",
       "      <th>First Party Collection/Use</th>\n",
       "      <th>International and Specific Audiences</th>\n",
       "      <th>Introductory/Generic</th>\n",
       "      <th>Policy Change</th>\n",
       "      <th>Practice not covered</th>\n",
       "      <th>Privacy contact information</th>\n",
       "      <th>Third Party Sharing/Collection</th>\n",
       "      <th>User Access, Edit and Deletion</th>\n",
       "      <th>User Choice/Control</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1017_sci-news.com</td>\n",
       "      <td>- details of your visits to our site including...</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1017_sci-news.com</td>\n",
       "      <td>- if you contact us, we may keep a record of t...</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             website                                            alltext  \\\n",
       "0  1017_sci-news.com  - details of your visits to our site including...   \n",
       "1  1017_sci-news.com  - if you contact us, we may keep a record of t...   \n",
       "\n",
       "   segment ID  Data Retention  Data Security  Do Not Track  \\\n",
       "0           3             0.0            0.0           0.0   \n",
       "1           2             1.0            0.0           0.0   \n",
       "\n",
       "   First Party Collection/Use  International and Specific Audiences  \\\n",
       "0                         1.0                                   0.0   \n",
       "1                         0.0                                   0.0   \n",
       "\n",
       "   Introductory/Generic  Policy Change  Practice not covered  \\\n",
       "0                   0.0            0.0                   0.0   \n",
       "1                   0.0            0.0                   0.0   \n",
       "\n",
       "   Privacy contact information  Third Party Sharing/Collection  \\\n",
       "0                          0.0                             0.0   \n",
       "1                          0.0                             0.0   \n",
       "\n",
       "   User Access, Edit and Deletion  User Choice/Control  \n",
       "0                             0.0                  0.0  \n",
       "1                             0.0                  0.0  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Dataset_ori = pd.read_csv('/Users/namrata/Documents/PSU/Thesis_research/Cybersecurity_framework/Data/OPP-115/data_to_gitlab/Cleaned_multilabel_dataset.txt',sep='\\t')\n",
    "del Dataset_ori['Other']\n",
    "Dataset_ori.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions to preprocess words\n",
    "import string\n",
    "def preprocess(words):\n",
    "    #we'll make use of python's translate function,that maps one set of characters to another\n",
    "    #we create an empty mapping table, the third argument allows us to list all of the characters \n",
    "    #to remove during the translation process\n",
    "    \n",
    "    #first we will try to filter out some  unnecessary data like tabs\n",
    "    table = str.maketrans('', '', '\\t')\n",
    "    table = str.maketrans('', '', '\\n')\n",
    "    words = [word.translate(table) for word in words]\n",
    "    \n",
    "    punctuations = (string.punctuation).replace(\"'\", \"\") \n",
    "    # the character: ' appears in a lot of stopwords and changes meaning of words if removed\n",
    "    #hence it is removed from the list of symbols that are to be discarded from the documents\n",
    "    trans_table = str.maketrans('', '', punctuations)\n",
    "    stripped_words = [word.translate(trans_table) for word in words]\n",
    "    \n",
    "    #some white spaces may be added to the list of words, due to the translate function & nature of our documents\n",
    "    #we remove them below\n",
    "    words = [str for str in stripped_words if str]\n",
    "    \n",
    "    #some words are quoted in the documents & as we have not removed ' to maintain the integrity of some stopwords\n",
    "    #we try to unquote such words below\n",
    "    p_words = []\n",
    "    for word in words:\n",
    "        if (word[0] and word[len(word)-1] == \"'\"):\n",
    "            word = word[1:len(word)-1]\n",
    "        elif(word[0] == \"'\"):\n",
    "            word = word[1:len(word)]\n",
    "        else:\n",
    "            word = word\n",
    "        p_words.append(word)\n",
    "    \n",
    "    words = p_words.copy()\n",
    "        \n",
    "    #we will also remove just-numeric strings as they do not have any significant meaning in text classification\n",
    "    words = [word for word in words if not word.isdigit()]\n",
    "    \n",
    "    #we will also remove single character strings\n",
    "    words = [word for word in words if not len(word) == 1]\n",
    "    \n",
    "    #after removal of so many characters it may happen that some strings have become blank, we remove those\n",
    "    words = [str for str in words if str]\n",
    "    \n",
    "    #we also normalize the cases of our words\n",
    "    words = [word.lower() for word in words]\n",
    "    \n",
    "    #we try to remove words with only 2 characters\n",
    "    words = [word for word in words if len(word) > 2]\n",
    "    \n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "StopWords = ['a', 'about', 'above', 'after', 'again', 'against', 'all', 'am', 'an', 'and', 'any', 'are', \"aren't\", 'as', 'at',\n",
    " 'be', 'because', 'been', 'before', 'being', 'below', 'between', 'both', 'but', 'by', \n",
    " 'can', \"can't\", 'cannot', 'could', \"couldn't\", 'did', \"didn't\", 'do', 'does', \"doesn't\", 'doing', \"don't\", 'down', 'during',\n",
    " 'each', 'few', 'for', 'from', 'further', \n",
    " 'had', \"hadn't\", 'has', \"hasn't\", 'have', \"haven't\", 'having', 'he', \"he'd\", \"he'll\", \"he's\", 'her', 'here', \"here's\",\n",
    " 'hers', 'herself', 'him', 'himself', 'his', 'how', \"how's\",\n",
    " 'i', \"i'd\", \"i'll\", \"i'm\", \"i've\", 'if', 'in', 'into', 'is', \"isn't\", 'it', \"it's\", 'its', 'itself',\n",
    " \"let's\", 'me', 'more', 'most', \"mustn't\", 'my', 'myself',\n",
    " 'no', 'nor', 'not', 'of', 'off', 'on', 'once', 'only', 'or', 'other', 'ought', 'our', 'ours' 'ourselves', 'out', 'over', 'own',\n",
    " 'same', \"shan't\", 'she', \"she'd\", \"she'll\", \"she's\", 'should', \"shouldn't\", 'so', 'some', 'such', \n",
    " 'than', 'that',\"that's\", 'the', 'their', 'theirs', 'them', 'themselves', 'then', 'there', \"there's\", 'these', 'they', \"they'd\", \n",
    " \"they'll\", \"they're\", \"they've\", 'this', 'those', 'through', 'to', 'too', 'under', 'until', 'up', 'very', \n",
    " 'was', \"wasn't\", 'we', \"we'd\", \"we'll\", \"we're\", \"we've\", 'were', \"weren't\", 'what', \"what's\", 'when', \"when's\", 'where',\n",
    " \"where's\", 'which', 'while', 'who', \"who's\", 'whom', 'why', \"why's\",'will', 'with', \"won't\", 'would', \"wouldn't\", \n",
    " 'you', \"you'd\", \"you'll\", \"you're\", \"you've\", 'your', 'yours', 'yourself', 'yourselves', \n",
    " 'one', 'two', 'three', 'four', 'five', 'six', 'seven', 'eight', 'nine', 'ten', 'hundred', 'thousand', '1st', '2nd', '3rd',\n",
    " '4th', '5th', '6th', '7th', '8th', '9th', '10th']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(words):\n",
    "    words = [word for word in words if not word in StopWords]\n",
    "    stop_words = list(set(stopwords.words('english')))\n",
    "    words = [word for word in words if not word in stop_words]\n",
    "    return words\n",
    "\n",
    "def tokenize_sentence(words):\n",
    "    words = preprocess(words)\n",
    "    words = remove_stopwords(words)\n",
    "    return words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"what's\", \"what is \", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"can't\", \"can not \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"i'm\", \"i am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r\"\\'scuse\", \" excuse \", text)\n",
    "    text = re.sub('\\W', ' ', text)\n",
    "    text = re.sub('\\s+', ' ', text)\n",
    "    text = text.strip(' ')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_clean_words(text):\n",
    "    text = clean_text(text)\n",
    "    tokens = word_tokenize(text)\n",
    "    clean_tokens =tokenize_sentence(tokens)\n",
    "    return clean_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['details',\n",
       " 'visits',\n",
       " 'site',\n",
       " 'including',\n",
       " 'limited',\n",
       " 'traffic',\n",
       " 'data',\n",
       " 'location',\n",
       " 'data',\n",
       " 'weblogs',\n",
       " 'communication',\n",
       " 'data']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "return_clean_words(Dataset_ori['alltext'].iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Word2Vec using all text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import SnowballStemmer\n",
    "import re\n",
    "from gensim import utils\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "import gensim.models as g\n",
    "import codecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_lists = []\n",
    "for s in Dataset_ori['alltext']:\n",
    "    all_lists.append(return_clean_words(s))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = Word2Vec(all_lists, min_count=1,size= 300,workers=10, window =10, sg = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dataset_ori[\"Id\"] = Dataset_ori['website'] + Dataset_ori['segment ID'].astype('str')\n",
    "labels_order = Dataset_ori.columns[3:-1]\n",
    "all_websites = Dataset_ori['website'].unique()\n",
    "web_dict = Counter(all_websites)\n",
    "\n",
    "selection_per_loop = dict()\n",
    "\n",
    "fraction =1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:  0\n",
      "Before upsampling:  (2381, 300) (2381, 12) (1193, 300) (1193, 12)\n",
      "After Up-sampling (45730, 300) (45730, 12) (1193, 300) (1193, 12)\n",
      "0.40135494139286476\n",
      "Fold:  1\n",
      "Before upsampling:  (2269, 300) (2269, 12) (1305, 300) (1305, 12)\n",
      "After Up-sampling (47964, 300) (47964, 12) (1305, 300) (1305, 12)\n",
      "0.39507307542296655\n",
      "Fold:  2\n",
      "Before upsampling:  (2368, 300) (2368, 12) (1206, 300) (1206, 12)\n",
      "After Up-sampling (45267, 300) (45267, 12) (1206, 300) (1206, 12)\n",
      "0.42809798780414593\n",
      "Fold:  3\n",
      "Before upsampling:  (2307, 300) (2307, 12) (1267, 300) (1267, 12)\n",
      "After Up-sampling (43497, 300) (43497, 12) (1267, 300) (1267, 12)\n",
      "0.3988382345001267\n",
      "Fold:  4\n",
      "Before upsampling:  (2484, 300) (2484, 12) (1090, 300) (1090, 12)\n",
      "After Up-sampling (55292, 300) (55292, 12) (1090, 300) (1090, 12)\n",
      "0.3881493071191556\n",
      "Fold:  5\n",
      "Before upsampling:  (2365, 300) (2365, 12) (1209, 300) (1209, 12)\n",
      "After Up-sampling (47120, 300) (47120, 12) (1209, 300) (1209, 12)\n",
      "0.4221855514842421\n",
      "Fold:  6\n",
      "Before upsampling:  (2282, 300) (2282, 12) (1292, 300) (1292, 12)\n",
      "After Up-sampling (45730, 300) (45730, 12) (1292, 300) (1292, 12)\n",
      "0.40662786671153345\n",
      "Fold:  7\n",
      "Before upsampling:  (2343, 300) (2343, 12) (1231, 300) (1231, 12)\n",
      "After Up-sampling (47940, 300) (47940, 12) (1231, 300) (1231, 12)\n",
      "0.37916853397002637\n",
      "Fold:  8\n",
      "Before upsampling:  (2302, 300) (2302, 12) (1272, 300) (1272, 12)\n",
      "After Up-sampling (49068, 300) (49068, 12) (1272, 300) (1272, 12)\n",
      "0.4101239654874023\n",
      "Fold:  9\n",
      "Before upsampling:  (2290, 300) (2290, 12) (1284, 300) (1284, 12)\n",
      "After Up-sampling (42897, 300) (42897, 12) (1284, 300) (1284, 12)\n",
      "0.3973827718785097\n"
     ]
    }
   ],
   "source": [
    "folds = 10\n",
    "\n",
    "lp = LabelPowerset()\n",
    "ros = RandomOverSampler(random_state=420)\n",
    "\n",
    "err=0\n",
    "for count in range(folds):\n",
    "# count=0\n",
    "    print('Fold: ',count)\n",
    "    itemminValue = min(web_dict.items(), key=lambda x: x[1])\n",
    "\n",
    "    min_val = itemminValue[1]\n",
    "    minimum_webs = list()\n",
    "    # print(min_val)\n",
    "    for key, value in web_dict.items():\n",
    "        if value == min_val:\n",
    "            minimum_webs.append(key)\n",
    "    random.shuffle(minimum_webs)\n",
    "\n",
    "    selected_test_webs = minimum_webs[0:40]\n",
    "\n",
    "    while len(selected_test_webs)<40:\n",
    "        min_val+=1\n",
    "        minimum_webs = list()\n",
    "        for key, value in web_dict.items():\n",
    "            if value == min_val:\n",
    "                minimum_webs.append(key)\n",
    "        random.shuffle(minimum_webs)\n",
    "        selected_test_webs = list(set(selected_test_webs)|set(minimum_webs[0:40-len(selected_test_webs)]))\n",
    "        # print('final len of test sites',len(selected_test_webs))\n",
    "\n",
    "    for tw in selected_test_webs:\n",
    "        web_dict[tw] += 1\n",
    "    selection_per_loop[count] = selected_test_webs\n",
    "\n",
    "    train_df = Dataset_ori[~Dataset_ori['website'].isin(selected_test_webs)]\n",
    "    test_df = Dataset_ori[Dataset_ori['website'].isin(selected_test_webs)]\n",
    "\n",
    "    train_df=train_df.sample(frac=fraction)\n",
    "    test_df=test_df.sample(frac=fraction)\n",
    "\n",
    "    train_text_ori = train_df['alltext'].tolist()\n",
    "    # train_text_ori = [' '.join(t.split()) for t in train_text_ori]\n",
    "    # train_text_ori = np.array(train_text_ori, dtype=object)[:, np.newaxis]\n",
    "    train_label_ori = train_df.values[:,3:-1].astype(int)\n",
    "\n",
    "    test_text = test_df['alltext'].tolist()\n",
    "    # test_text = [' '.join(t.split()) for t in test_text]\n",
    "    # test_text = np.array(test_text, dtype=object)[:, np.newaxis]\n",
    "    test_label = test_df.values[:,3:-1].astype(int)\n",
    "    \n",
    "\n",
    "    X1 = []\n",
    "    for i in range(len(train_text_ori)):\n",
    "        vec_inp = return_clean_words(train_text_ori[i])\n",
    "        vector_S1 = []\n",
    "        for s in vec_inp:\n",
    "            try:\n",
    "                vector_S1.append(m[s])\n",
    "            except:\n",
    "                continue\n",
    "        vector1 = np.mean(vector_S1, axis=0,keepdims=True)\n",
    "        \n",
    "        if vector1.shape != (1,300):\n",
    "            print('whole vec empty',err)\n",
    "            err+=1\n",
    "            vector1 = np.random.rand(1,300)\n",
    "        X1.append(vector1)\n",
    "    train_X = np.vstack(X1)\n",
    "\n",
    "    X2 = []\n",
    "    \n",
    "    for i in range(len(test_text)):\n",
    "        vec_inp = return_clean_words(test_text[i])\n",
    "        vector_S1 = []\n",
    "        for s in vec_inp:\n",
    "            try:\n",
    "                vector_S1.append(m[s])\n",
    "            except:\n",
    "                continue\n",
    "        vector1 = np.mean(vector_S1, axis=0,keepdims=True)\n",
    "        \n",
    "        if vector1.shape != (1,300):\n",
    "            print('whole vec empty',err)\n",
    "            err+=1\n",
    "            vector1 = np.random.rand(1,300)\n",
    "        X2.append(vector1)\n",
    "        \n",
    "    test_X = np.vstack(X2)\n",
    "\n",
    "    # print(test_X.shape)\n",
    "\n",
    "    train_X_norm = min_max_scaler.fit_transform(train_X)\n",
    "    test_X_norm = min_max_scaler.fit_transform(test_X)\n",
    "\n",
    "    print('Before upsampling: ',train_X_norm.shape,train_label_ori.shape,test_X_norm.shape,test_label.shape)\n",
    "\n",
    "    yt = lp.transform(train_label_ori)\n",
    "    train_X_norm_up, y_resampled = ros.fit_sample(train_X_norm, yt)\n",
    "    # train_text = train_text_ori\n",
    "    # train_label = train_label_ori\n",
    "    train_label = lp.inverse_transform(y_resampled).toarray()\n",
    "\n",
    "    print('After Up-sampling',train_X_norm_up.shape,train_label.shape,test_X_norm.shape,test_label.shape)\n",
    "    \n",
    "    NB_pipeline = Pipeline([\n",
    "                    #('tfidf', TfidfVectorizer(stop_words=stop_words)),\n",
    "                    ('clf', OneVsRestClassifier(MultinomialNB(\n",
    "                        fit_prior=True, class_prior=None))),\n",
    "                ])        \n",
    "\n",
    "    all_pred_res = []\n",
    "    for category in range(train_label.shape[1]):\n",
    "        NB_pipeline.fit(train_X_norm_up, train_label[:,category].reshape((train_label.shape[0], 1)))\n",
    "        preds = NB_pipeline.predict(test_X_norm)\n",
    "        all_pred_res.append(preds)\n",
    "\n",
    "    # print(len(all_pred_res))\n",
    "    pred_binary = np.asarray(all_pred_res).T\n",
    "\n",
    "    # print(pred_binary.shape)\n",
    "\n",
    "    result_scores = pd.DataFrame()\n",
    "    avg_f1 = []\n",
    "    for category in range(train_label.shape[1]):\n",
    "        label_name = labels_order[category]\n",
    "\n",
    "    #     lr_precision, lr_recall, _ = precision_recall_curve(test_label[:,category].astype('float'), pre_save_preds[:,category])\n",
    "        lr_f1 = f1_score(test_label[:,category].astype(int), pred_binary[:,category])#, auc(lr_recall, lr_precision)\n",
    "\n",
    "        if sum(test_label[:,category])==0 and sum(pred_binary[:,category])==0:\n",
    "            tp=0\n",
    "            fp=0\n",
    "            tn=len(pred_binary[:,category])\n",
    "            fn=0\n",
    "        else:\n",
    "            tn, fp, fn, tp = confusion_matrix(test_label[:,category].astype(int), pred_binary[:,category]).ravel()\n",
    "\n",
    "        if tp+fp==0:\n",
    "            precision = 0\n",
    "        else:\n",
    "            precision = float(tp)/float(tp+fp)\n",
    "\n",
    "        if tp+fn==0:\n",
    "            recall = 0\n",
    "        else:\n",
    "            recall = float(tp)/float(tp+fn)\n",
    "\n",
    "        avg_f1.append(lr_f1)\n",
    "        df = pd.DataFrame([{'Fold':count,'category':label_name,'tp':tp,'fp':fp,'tn':tn,'fn':fn,'recall':recall,'precision':precision,'f1':lr_f1}])\n",
    "\n",
    "        result_scores = result_scores.append(df)\n",
    "#     print(result_scores)\n",
    "    print(np.mean(avg_f1))\n",
    "    if count==0:\n",
    "        result_scores.to_csv('.../Word2Vec_OPP_res_ori_upsampled_self_trained.csv',header=True,mode='a',sep='\\t')\n",
    "    else:\n",
    "        result_scores.to_csv('.../Word2Vec_OPP_res_ori_upsampled_self_trained.csv',header=False,mode='a',sep='\\t')\n",
    "\n",
    "    count+=1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
